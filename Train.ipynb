{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a43afc-b303-45fe-ac99-a73fd64b3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b02011-9f04-426c-aaca-eb8bce49f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:  # default parameters from the paper and official implementation\n",
    "    learning_rate: float = 0.01\n",
    "    num_epochs: int = 200\n",
    "    weight_decay: float = 5e-4\n",
    "    num_warmup_steps: int = 0\n",
    "    save_each_epoch: bool = False\n",
    "    output_dir: str = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06e41542-af82-46bd-92df-719c58b7c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, features, train_labels, val_labels, additional_matrix, device, run_config, log=True):\n",
    "        self.model = self.model.to(device)\n",
    "        features = features.to(device)\n",
    "        train_labels = train_labels.to(device)\n",
    "        additional_matrix = additional_matrix.to(device)  # adjacency or laplacian matrix depending on the model\n",
    "\n",
    "        optimizer = Adam(self.model.parameters(), lr=run_config.learning_rate, weight_decay=run_config.weight_decay)\n",
    "\n",
    "        # https://huggingface.co/transformers/_modules/transformers/optimization.html#get_linear_schedule_with_warmup\n",
    "        def lr_lambda(current_step: int):\n",
    "            if current_step < run_config.num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, run_config.num_warmup_steps))\n",
    "            return max(0.0, float(run_config.num_epochs - current_step) /\n",
    "                       float(max(1, run_config.num_epochs - run_config.num_warmup_steps)))\n",
    "\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "        if log:\n",
    "            print(\"Training started:\")\n",
    "            print(f\"\\tNum Epochs = {run_config.num_epochs}\")\n",
    "\n",
    "        best_loss, best_model_accuracy = float(\"inf\"), 0\n",
    "        best_model_state_dict = None\n",
    "        train_iterator = tqdm(range(0, int(run_config.num_epochs)), desc=\"Epoch\")\n",
    "        for epoch in train_iterator:\n",
    "            self.model.train()\n",
    "            outputs = self.model(features, additional_matrix, train_labels)\n",
    "            loss = outputs[1]\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            val_loss, val_accuracy = self.evaluate(features, val_labels, additional_matrix, device)\n",
    "            train_iterator.set_description(f\"Training loss = {loss.item():.4f}, \"\n",
    "                                           f\"val loss = {val_loss:.4f}, val accuracy = {val_accuracy:.2f}\")\n",
    "\n",
    "            save_best_model = val_loss < best_loss\n",
    "            if save_best_model:\n",
    "                best_loss = val_loss\n",
    "                best_model_accuracy = val_accuracy\n",
    "                best_model_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "            if save_best_model or run_config.save_each_epoch or epoch + 1 == run_config.num_epochs:\n",
    "                output_dir = os.path.join(run_config.output_dir, f\"Epoch_{epoch + 1}\")\n",
    "                self.save(output_dir)\n",
    "        if log:\n",
    "            print(f\"Best model val CE loss = {best_loss:.4f}, best model val accuracy = {best_model_accuracy:.2f}\")\n",
    "        # reloads the best model state dict, bit hacky :P\n",
    "        self.model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "    def evaluate(self, features, test_labels, additional_matrix, device):\n",
    "        features = features.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "        additional_matrix = additional_matrix.to(device)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        outputs = self.model(features, additional_matrix, test_labels)\n",
    "        ce_loss = outputs[1].item()\n",
    "\n",
    "        ignore_label = nn.CrossEntropyLoss().ignore_index\n",
    "        predicted_label = torch.max(outputs[0], dim=1).indices[test_labels != ignore_label]\n",
    "        true_label = test_labels[test_labels != -100]\n",
    "        accuracy = torch.mean((true_label == predicted_label).type(torch.FloatTensor)).item()\n",
    "\n",
    "        return ce_loss, accuracy\n",
    "\n",
    "    def save(self, output_dir):\n",
    "        if not os.path.isdir(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        model_path = os.path.join(output_dir, \"model.pth\")\n",
    "        torch.save(self.model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
